{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evacuation_RL_0928_One_Class_126_90.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNe5cdtg+75kV5a1RrnZQ7p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bravoila/MachineLearning/blob/master/Evacuation_RL_0928_One_Class_126_90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXfVWLMxxEI0",
        "outputId": "dc04f502-fb03-4b3a-9fe2-4c66f22300ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "pip install rlcard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rlcard in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from rlcard) (1.18.5)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from rlcard) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from rlcard) (20.4)\n",
            "Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from rlcard) (7.0.0)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from rlcard) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard) (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhGcGdwdxICT",
        "outputId": "a5401895-ad2c-47ca-8242-fbfb96f2fb47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "pip install rlcard[tensorflow]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rlcard[tensorflow] in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (7.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (20.4)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (3.2.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (1.18.5)\n",
            "Requirement already satisfied: tensorflow<2.0,>=1.14; extra == \"tensorflow\" in /usr/local/lib/python3.6/dist-packages (from rlcard[tensorflow]) (1.15.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard[tensorflow]) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard[tensorflow]) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard[tensorflow]) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard[tensorflow]) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard[tensorflow]) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.10.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.15.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.32.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (50.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14; extra == \"tensorflow\"->rlcard[tensorflow]) (3.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCm-mXNVJt4U",
        "outputId": "3baa0bdc-850e-48dc-8359-0b8ba866a834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Requirement already satisfied: stable-baselines[mpi]==2.10.0 in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.2)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grxcnf0WJ7VX"
      },
      "source": [
        "Code for environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj7EwdXCJ4Hs"
      },
      "source": [
        "#from google.colab import files\n",
        "#import io\n",
        "#uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1BceU4TKZ2O",
        "cellView": "both"
      },
      "source": [
        "#import io\n",
        "#io.BytesIO(uploaded['patient_generattion2.xlsx'])\n",
        "#@title Code for Environment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUxRL33-4CWG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBIND4ZhJ6f7",
        "cellView": "both"
      },
      "source": [
        "#@title Code for Environment\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Dec 17 11:46:46 2020\n",
        "\n",
        "@author: machaolun\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Unit: Minute\n",
        "\"\"\"\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "# import tkinter as tk\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from collections import deque\n",
        "from heapq import heappush,heappop\n",
        "import csv\n",
        "import copy\n",
        "from collections import deque\n",
        "\n",
        "random.seed(2020)\n",
        "# This class is to generate patients\n",
        "total_simulation_time = 10000\n",
        "\n",
        "\n",
        "def SURV_PROB_CAL(my_facility, my_patype, my_total_time, surv_num):\n",
        "        # survival probablity\n",
        "    # Resource-Based Patient Prioritization in Mass-Casualty Incidents\n",
        "    # first is the worst case and the fifth is the optimistic case    \n",
        "    # Immediate         Delayed\n",
        "    # β0,I β1,I β2,I    β0,D β1,D β2,D\n",
        "    # 0.09,17  ,1.01    0.57,61   ,2.03\n",
        "    # 0.15,28  ,1.38    0.65,86   ,2.11\n",
        "    # 0.24,47  ,1.30    0.76,138  ,2.17\n",
        "    # 0.40,59  ,1.47    0.77,140  ,2.29\n",
        "    # 0.56,91  ,1.58    0.81,160  ,1.41\n",
        "    if surv_num == 1:\n",
        "        # set para\n",
        "        # Senerio 1, assume no different between ACF and hospital\n",
        "        if my_facility == 1 and my_patype == 0:\n",
        "            \"\"\"Hospital, immediate\"\"\"\n",
        "            Beta = [0.09,17  ,1.01]\n",
        "        elif my_facility == 1 and my_patype == 1:\n",
        "            \"\"\"Hospital, delayed\"\"\"\n",
        "            Beta = [0.57,61 ,2.03]\n",
        "        elif my_facility == 2 and my_patype == 0:\n",
        "            \"\"\"ACF, immediate\"\"\"\n",
        "            Beta = [0.09,17  ,1.01]\n",
        "        elif my_facility == 2 and my_patype == 1:\n",
        "            \"\"\"ACF, delayed\"\"\"\n",
        "            Beta = [0.57,61   ,2.03]\n",
        "    elif surv_num == 2:       \n",
        "        # # Senerio 2, assume no different between ACF and hospital\n",
        "        if my_facility == 1 and my_patype == 0:\n",
        "            \"\"\"Hospital, immediate\"\"\"\n",
        "            Beta = [0.15,28  ,1.38]\n",
        "        elif my_facility == 1 and my_patype == 1:\n",
        "            \"\"\"Hospital, delayed\"\"\"\n",
        "            Beta = [0.65,86   ,2.11]\n",
        "        elif my_facility == 2 and my_patype == 0:\n",
        "            \"\"\"ACF, immediate\"\"\"\n",
        "            Beta = [0.15,28  ,1.38]\n",
        "        elif my_facility == 2 and my_patype == 1:\n",
        "            \"\"\"ACF, delayed\"\"\"\n",
        "            Beta = [0.65,86   ,2.11]\n",
        "            \n",
        "    elif surv_num == 3:\n",
        "        \n",
        "        # # Senerio 3, assume no different between ACF and hospital\n",
        "        if my_facility == 1 and my_patype == 0:\n",
        "            \"\"\"Hospital, immediate\"\"\"\n",
        "            Beta = [0.24,47  ,1.30]\n",
        "        elif my_facility == 1 and my_patype == 1:\n",
        "            \"\"\"Hospital, delayed\"\"\"\n",
        "            Beta = [0.76,138  ,2.17]\n",
        "        elif my_facility == 2 and my_patype == 0:\n",
        "            \"\"\"ACF, immediate\"\"\"\n",
        "            Beta = [0.24,47  ,1.30]\n",
        "        elif my_facility == 2 and my_patype == 1:\n",
        "            \"\"\"ACF, delayed\"\"\"\n",
        "            Beta = [0.76,138  ,2.17]\n",
        "    elif surv_num == 4:\n",
        "            \n",
        "        # # Senerio 4, assume no different between ACF and hospital\n",
        "        if my_facility == 1 and my_patype == 0:\n",
        "            \"\"\"Hospital, immediate\"\"\"\n",
        "            Beta = [0.40,59  ,1.47]\n",
        "        elif my_facility == 1 and my_patype == 1:\n",
        "            \"\"\"Hospital, delayed\"\"\"\n",
        "            Beta = [0.77,140  ,2.29]\n",
        "        elif my_facility == 2 and my_patype == 0:\n",
        "            \"\"\"ACF, immediate\"\"\"\n",
        "            Beta = [0.40,59  ,1.47]\n",
        "        elif my_facility == 2 and my_patype == 1:\n",
        "            \"\"\"ACF, delayed\"\"\"\n",
        "            Beta = [0.77,140  ,2.29]            \n",
        "    elif surv_num == 5:\n",
        "        # Senerio 5, assume no different between ACF and hospital            \n",
        "        if my_facility == 1 and my_patype == 0:\n",
        "            \"\"\"Hospital, immediate\"\"\"\n",
        "            Beta = [0.56,91  ,1.58]\n",
        "        elif my_facility == 1 and my_patype == 1:\n",
        "            \"\"\"Hospital, delayed\"\"\"\n",
        "            Beta = [0.81,160  ,1.41]\n",
        "        elif my_facility == 2 and my_patype == 0:\n",
        "            \"\"\"ACF, immediate\"\"\"\n",
        "            Beta = [0.56,91  ,1.58]\n",
        "        elif my_facility == 2 and my_patype == 1:\n",
        "            \"\"\"ACF, delayed\"\"\"\n",
        "            Beta = [0.81,160  ,1.41]        \n",
        "    \n",
        "    # 0.9 is ACF discount factor\n",
        "    if my_facility == 1:\n",
        "        SP = Beta[0]/((my_total_time/Beta[1]) ** Beta[2] + 1.0)\n",
        "    else:\n",
        "        SP = 0.8 * Beta[0]/((my_total_time/Beta[1]) ** Beta[2] + 1.0)\n",
        "    return SP\n",
        "\n",
        "\n",
        "def eva_perform(env, num):\n",
        "    ''' Evaluate he performance of the agents in the environment\n",
        "    Args:\n",
        "        env (Env class): The environment to be evaluated.\n",
        "        num (int): The number of games to play.\n",
        "    Returns:\n",
        "        A list of avrage payoffs for each player\n",
        "    '''\n",
        "    payoffs = 0\n",
        "    counter = 0\n",
        "\n",
        "    while counter < num:\n",
        "        tj = env.run(1, is_training=False)\n",
        "        return_ = pd.DataFrame(tj)\n",
        "        payoffs += sum(return_[2])\n",
        "        counter += 1\n",
        "\n",
        "    payoffs /= counter\n",
        "    \n",
        "    return payoffs\n",
        "\n",
        "class Event(object):\n",
        "    # num_of_events = 0\n",
        "    def __init__(self,generate_time, ptype, surv_num):\n",
        "        self.generate_time = generate_time\n",
        "        self.trans_time = 0.2\n",
        "        self.arrival_time = 0.3\n",
        "        self.start_service_time = 0.4\n",
        "        self.service_time = 0.5\n",
        "        self.departure_time = 0.6\n",
        "        self.survival_prob_res = 0\n",
        "        self.facility = 0.7\n",
        "        self.patype = ptype\n",
        "        self.status = \"default\"\n",
        "        self.surv_num = surv_num\n",
        "        # severe , minor\n",
        "        # Event.num_of_events += 1\n",
        "    def total_time(self):\n",
        "        return self.departure_time - self.generate_time\n",
        "    \n",
        "    def sojourn_time(self):\n",
        "        return self.departure_time - self.arrival_time\n",
        "\n",
        "    def waiting_time(self):\n",
        "        # waiting time in queue\n",
        "        return self.sojourn_time() - self.service_time\n",
        "    \n",
        "    def survival_prob(self):\n",
        "        my_total_time = self.total_time()\n",
        "        SP = SURV_PROB_CAL(self.facility, self.patype, my_total_time, self.surv_num)\n",
        "        self.survival_prob_res = SP\n",
        "        return SP\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.generate_time}, {self.trans_time},{self.arrival_time}, {self.start_service_time}, {self.service_time}, {self.departure_time}, {self.survival_prob_res}, {self.facility}, {self.patype} \\n\"\n",
        "    def get_info(self):\n",
        "        #print([self.generate_time, self.trans_time, self.arrival_time, self.start_service_time, self.service_time, self.departure_time, self.facility, self.patype, self.status])\n",
        "        return [self.generate_time, self.trans_time, self.arrival_time, self.start_service_time, self.service_time, self.departure_time, self.survival_prob_res, self.facility, self.patype, self.status]\n",
        "    \n",
        "\"\"\"########################################################################\"\"\"\n",
        "\n",
        "class Server(object):\n",
        "    def __init__(self, c, status, mu0, mu1, tt):\n",
        "        # self.jobs = jobs\n",
        "        self.c = c\n",
        "        self.status = status\n",
        "        self.mu0 = mu0\n",
        "        self.mu1 = mu1\n",
        "        self.tt = tt\n",
        "        self.server_tnext = 0\n",
        "\n",
        "        self.num_busy = 0  # number of busy servers\n",
        "        self.stack = []    # store the patient info in the server\n",
        "        self.queue = []\n",
        "        self.served_jobs = [] \n",
        "\n",
        "    def handle_arrival(self, time, job):\n",
        "        # time here refers to arrival time\n",
        "        # print(\"arrival_job\", job)\n",
        "        # input(\"arrival\")\n",
        "    \n",
        "        if self.num_busy < self.c:\n",
        "            if len(self.queue) == 0:\n",
        "                # print(\"handle_arrival1 \")\n",
        "                self.start_service(time, job)\n",
        "        else:\n",
        "            job.status = \"queue\"\n",
        "            heappush(self.queue, ((job.patype, job.arrival_time), job))\n",
        "            \n",
        "        # print(\"handle_arrival_done \")\n",
        "\n",
        "    def start_service(self, time, job):\n",
        "        # time here refers to start service time\n",
        "        self.num_busy += 1  # server becomes busy.\n",
        "        job.start_service_time = time\n",
        "        job.departure_time = time + job.service_time\n",
        "        \n",
        "        job.status = \"stack\"\n",
        "        heappush(self.stack, (job.departure_time, job))\n",
        "        \n",
        "        # print(\"service_job \", job)\n",
        "        # input(\"service\")\n",
        "        \n",
        "        if job.departure_time <= self.server_tnext:\n",
        "            # print(\"start_service1 \")\n",
        "            self.handle_departure()\n",
        "            \n",
        "        \n",
        "    def handle_departure(self):\n",
        "        # time here refers to departure time \n",
        "        # self.stack.sort(reverse=True)\n",
        "        try:\n",
        "            while self.stack[0][0] <= self.server_tnext:\n",
        "                # print(\"while \", self.stack[0][0], self.server_tnext)\n",
        "                prev_depart = copy.deepcopy(self.stack[0][0])\n",
        "                self.num_busy -= 1\n",
        "                self.stack[0][1].status = \"released\"\n",
        "                next_done = heappop(self.stack)\n",
        "                next_done[1].survival_prob()\n",
        "                self.served_jobs.append(next_done)\n",
        "                \n",
        "                # print(\"next_done \",next_done)\n",
        "                # input(\"depature\")\n",
        "                \n",
        "                if self.queue and self.num_busy < self.c:\n",
        "                    if self.queue[0][1].arrival_time < self.server_tnext:\n",
        "                        # print(\"queue\")\n",
        "                        # print(\"self.server_tnext \", self.server_tnext)\n",
        "                        # print(\"prev_depart \", prev_depart)\n",
        "                        next_job = heappop(self.queue)\n",
        "                        # print(\"next_job! \", next_job)\n",
        "                        \n",
        "                        # input(\"here1\")\n",
        "                        # print(\"arr!\", next_job[1].arrival_time)\n",
        "                        # print(\"prev!\",prev_depart)\n",
        "                        # input(\"here2\")\n",
        "                        self.start_service(max(next_job[1].arrival_time, prev_depart),next_job[1])\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    def travel_time(self):\n",
        "        return self.tt\n",
        "        # return random.expovariate(self.tt)\n",
        "        \n",
        "        # return self.tt\n",
        "        \n",
        "    def service_time(self, patient_type):\n",
        "        if patient_type == 0:\n",
        "            return random.expovariate(self.mu0)\n",
        "        else:\n",
        "            return random.expovariate(self.mu1)\n",
        "            \n",
        "    def getSize(self, tnow):\n",
        "        try:\n",
        "            count_queue0 = 0\n",
        "            count_queue1 = 0\n",
        "            copy_queue = copy.deepcopy(self.queue)\n",
        "            for item in copy_queue:\n",
        "                if item[1].arrival_time <= tnow: \n",
        "                    if item[1].patype == 0:\n",
        "                        count_queue0 += 1\n",
        "                    else:\n",
        "                        count_queue1 += 1      \n",
        "                    \n",
        "            return [count_queue0, count_queue1]\n",
        "        \n",
        "        except:\n",
        "            return [0,0]\n",
        "    \n",
        "    \n",
        "    \"\"\"need to optimize code \"\"\"\n",
        "    def getBusyServer(self, tnow):\n",
        "        count_busyserver = 0\n",
        "        copy_stack = copy.deepcopy(self.stack) + copy.deepcopy(self.served_jobs) \n",
        "        # print(\"copy_stack \", copy_stack)\n",
        "        # print(\"stack \", self.stack\n",
        "        \n",
        "        try:\n",
        "            for item in copy_stack:\n",
        "                # print(\"item \",item[1])\n",
        "                if item[1].departure_time >= tnow and item[1].start_service_time <= tnow and count_busyserver < self.c:\n",
        "                    count_busyserver += 1\n",
        "                # print(\"count_busyserver\", count_busyserver)\n",
        "                \n",
        "            return [count_busyserver]\n",
        "        \n",
        "        except:\n",
        "            return [0]\n",
        "    \n",
        "    \n",
        "\"\"\"########################################################################\"\"\"\n",
        "\n",
        "def AssignType():\n",
        "    # set para\n",
        "    p = 0\n",
        "    myassign = np.random.binomial(1,p)\n",
        "    if myassign == 1:\n",
        "        patype = 0\n",
        "    else:\n",
        "        patype = 1\n",
        "    \n",
        "    return patype\n",
        "\n",
        "\"\"\"########################################################################\"\"\"\n",
        "\n",
        "class Env(gym.Env):\n",
        "    def __init__(self, tnow):\n",
        "        self.tnow = tnow\n",
        "        self.gen_patient_time_list = []\n",
        "        self.tgen = 0\n",
        "        self.tgen_next = 0\n",
        "        self.arrival_list1 = []\n",
        "        self.arrival_list2 = []\n",
        "        self.arrival_time = self.tnow\n",
        "        self.surv_num = 5\n",
        "        # Define constants for clearer code\n",
        "        self.S1 = 0\n",
        "        self.S2 = 1\n",
        "        self.action_num = 2\n",
        "        self.action_space = spaces.Discrete(self.action_num)\n",
        "\n",
        "        \n",
        "        \"\"\"['gen_time', 'num_to_hos', 'num_to_acf', \n",
        "        'ql10' ,'ql11', 'ql20', 'ql21', 'busy_1', 'busy_2','served_1', 'served_2', 'rewards'])\"\"\"    \n",
        "        \"\"\" obs space\"\"\"\n",
        "        \"\"\" ['num_to_hos', 'num_to_acf', 'ql10' ,'ql11', 'ql20', 'ql21', 'busy_1', 'busy_2', \n",
        "        'waiting time at hos', ' service at hos', 'waiting time at acf', ' service at acf'])\"\"\"\n",
        "        \n",
        "        self.obs_low = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "        self.obs_high = np.array([15, 15, 20, 20, 20, 20, 8, 4, 1000, 1000, 1000, 1000])\n",
        "        # self.observation_space = spaces.Box(self.obs_low, self.obs_high, dtype=np.int32)\n",
        "        \n",
        "        self.tgen_list = []\n",
        "        # pd.DataFrame(columns = ['gen_time', 'num_to_hos', 'num_to_acf', \n",
        "        # 'ql10' ,'ql11', 'ql20', 'ql21', 'busy_1', 'busy_2','served_1', 'served_2', 'rewards'])\n",
        "        self.rewards_records = {}\n",
        "        self.obs_records = {}\n",
        "        self.action_records = {}\n",
        "        self.wait_records1 = {}  # queuing_time\n",
        "        self.service_records1 = {} #service_time\n",
        "        self.wait_records2 = {}  # queuing_time\n",
        "        self.service_records2 = {}\n",
        "\n",
        "        \"\"\"Server Settings\"\"\"\n",
        "        \"\"\" mu_tt = log(x) \"\"\"\n",
        "        \"\"\"pdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2)) / (x * sigma * np.sqrt(2 * np.pi)))\"\"\"\n",
        "\n",
        "        # set para server\n",
        "\n",
        "        self.server1_rate0 = 1/90  \n",
        "        self.server1_rate1 = 1/30\n",
        "        \"\"\" mean value, not mu\"\"\"\n",
        "        self.server1_tt = 90\n",
        "        \n",
        "        self.server2_rate0 = 1/90\n",
        "        self.server2_rate1 = 1/30\n",
        "        \n",
        "        \"\"\" mean value, not mu\"\"\"\n",
        "        self.server2_tt = 10\n",
        "        \n",
        "        # __init__(self, c, status, mu0, mu1, tt):\n",
        "        self.server1 = Server(12, 'open',self.server1_rate0,self.server1_rate1,self.server1_tt)\n",
        "        self.server2 = Server(6, 'open', self.server2_rate0, self.server2_rate1,self.server2_tt)\n",
        "        self.s2_isopen = True   \n",
        "        self.simulation_time = total_simulation_time\n",
        "        \n",
        "        \n",
        "    #     \"\"\" use pre-generated data\"\"\"\n",
        "    # def Get_Time(self, myi):\n",
        "    #     nlength = 50\n",
        "    #     a = pd.read_excel(\"/Users/machaolun/Documents/GitHub/EvacuationMDP/Evacuation_code/RL/patient_generattion2.xlsx\",\n",
        "    #               index_col=0)\n",
        "    #     b = list(a.iloc[myi,:nlength])\n",
        "    #     self.gen_patient_time_list = deque(b)\n",
        "        \n",
        "        \n",
        "        \"\"\" use generated data\"\"\"\n",
        "    def Get_Time(self, myi):\n",
        "        total_num_patient = 50\n",
        "        arrive_lambda = 1/5.0\n",
        "        mylist = []\n",
        "        t= 0\n",
        "        \"\"\"Generate Casulties\"\"\"\n",
        "        for j in range(total_num_patient):\n",
        "            t+= random.expovariate(arrive_lambda)\n",
        "            mylist.append(t)   \n",
        "            \n",
        "        self.gen_patient_time_list = deque(mylist)   \n",
        "    # def GenerateTime(self):\n",
        "    #     \"\"\"\n",
        "    #     Arrival rate per min\n",
        "    #     Set Arrival Rate (time inhomogeneous) Here\n",
        "    #     \"\"\"\n",
        "    #     # ref: https://zhuanlan.zhihu.com/p/28785318\n",
        "    #     t = 0\n",
        "    #     I = 0\n",
        "    #     s = []\n",
        "        \n",
        "    #     max_simulate_time = 1000\n",
        "        \n",
        "    #     while t <= max_simulate_time:\n",
        "    #         u = np.random.uniform(1e-8,1)\n",
        "    #         t = t - np.log(u)\n",
        "    #         if t <= max_simulate_time:\n",
        "    #             u1 = np.random.uniform(1e-8,1)\n",
        "    #             if u1 <= inhop(t):\n",
        "    #                 I = I + 1\n",
        "    #                 s.append(t)\n",
        "                    \n",
        "    #     # time when event happens\n",
        "    #     self.gen_patient_time_list = deque(s)        \n",
        "    \n",
        "    \"\"\" Open new ACF, the resource comes from hospital\"\"\"        \n",
        "    # def take_action(self, Action):\n",
        "    #     if (not self.s2_isopen) and Action == \"open\":\n",
        "    #         original_num_server = copy.deepcopy(self.server1.c)\n",
        "    #         self.s2_isopen = True\n",
        "            \n",
        "    #         # set para\n",
        "    #         precent_move = 0.2\n",
        "    #         self.server1.c = round((1 - precent_move)*original_num_server)\n",
        "    #         self.server2.c = int(precent_move * original_num_server) + 2 \n",
        "    #         self.server2.status = 'open'\n",
        "            \n",
        "    #         print(\"!!!!!!!!!!!!!!!\\n\",self.server1.c, self.server2.c)\n",
        "    #     else:\n",
        "    #         pass \n",
        "        \n",
        "    def step(self, Action):\n",
        "        \n",
        "        try:\n",
        "            self.tgen = self.gen_patient_time_list.popleft()\n",
        "            try:\n",
        "                self.tgen_next = self.gen_patient_time_list[0]\n",
        "            except:\n",
        "                self.tgen_next = 100000\n",
        "            # action records\n",
        "            self.action_records[self.tgen] = Action\n",
        "            \n",
        "            patient = Event(self.tgen, AssignType(), self.surv_num)\n",
        "\n",
        "            #  self.s2_isopen and         \n",
        "            if Action == self.S2:    \n",
        "                patient.trans_time = self.server2.travel_time()\n",
        "                # self.server2.tnext = min(self.tgen + patient.travel_time , self.simulation_time)\n",
        "                patient.facility = 2\n",
        "                patient.arrival_time = patient.generate_time + patient.trans_time\n",
        "                patient.service_time = self.server2.service_time(patient.patype)\n",
        "                heappush(self.arrival_list2, ((patient.arrival_time), patient))\n",
        "                \n",
        "            elif Action == self.S1:\n",
        "                patient.trans_time = self.server1.travel_time()\n",
        "                # self.server1.tnext = min(self.tgen + patient.travel_time, self.simulation_time)\n",
        "                patient.facility = 1\n",
        "                patient.arrival_time = patient.generate_time + patient.trans_time\n",
        "                patient.service_time = self.server1.service_time(patient.patype) \n",
        "                heappush(self.arrival_list1, ((patient.arrival_time), patient))\n",
        "                                \n",
        "            else:\n",
        "                raise ValueError(\"Received invalid action={} which is not part of the action space\".format(Action))\n",
        "        \n",
        "        except:\n",
        "            self.tgen_next = 100001\n",
        "            \n",
        "        try:\n",
        "            while self.arrival_list1[0][0] <= self.tgen_next:\n",
        "                try:\n",
        "                    self.server1.server_tnext = min(self.arrival_list1[0][0], self.tgen_next)\n",
        "                except:\n",
        "                    self.server1.server_tnext = self.tgen_next\n",
        "                    \n",
        "                self.server1.handle_departure()\n",
        "                mypatient_s1 = heappop(self.arrival_list1)\n",
        "                self.server1.handle_arrival(mypatient_s1[1].arrival_time, mypatient_s1[1])\n",
        "                \n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        self.server1.server_tnext = self.tgen_next    \n",
        "        self.server1.handle_departure()\n",
        "        \n",
        "        if self.s2_isopen:\n",
        "            try:\n",
        "                while self.arrival_list2[0][0] <= self.tgen_next:\n",
        "    \n",
        "                    try:\n",
        "                        self.server2.server_tnext = min(self.arrival_list2[0][0], self.tgen_next)\n",
        "                    except:\n",
        "                        self.server2.server_tnext = self.tgen_next\n",
        "                        \n",
        "                    self.server2.handle_departure()\n",
        "                    \n",
        "                    mypatient_s2 = heappop(self.arrival_list2)\n",
        "                    self.server2.handle_arrival(mypatient_s2[1].arrival_time, mypatient_s2[1])\n",
        "                    \n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            self.server2.server_tnext = self.tgen_next\n",
        "            self.server2.handle_departure()\n",
        "                \n",
        "        \"\"\" gen_time, num to hos, num to acf, \n",
        "        [ql10,ql11], [ql20, ql21],\n",
        "        busy server1, busy 2 \"\"\"        \n",
        "        \"\"\"['gen_time', 'num_to_hos', 'num_to_acf', \n",
        "        'ql10' ,'ql11', 'ql20', 'ql21', 'busy_1', 'busy_2','served_1', 'served_2', 'rewards'])\"\"\"    \n",
        "        \n",
        "        # print(\"self.tgen_next\", self.tgen_next)\n",
        "        if self.s2_isopen:\n",
        "            Legal_Action = [0,1]\n",
        "        else:\n",
        "            Legal_Action = [0,1]\n",
        "\n",
        "        # for the first one\n",
        "        if len(self.gen_patient_time_list) == 49:\n",
        "            self.tgen_list.append([self.tgen, Legal_Action])\n",
        "            self.obs_records[self.tgen] = np.array([self.tgen, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0 ]) \n",
        "\n",
        "            #action record is above \n",
        "            \n",
        "        all_served = self.server1.served_jobs + self.server2.served_jobs\n",
        "        \n",
        "       \n",
        "        for jobs in all_served:\n",
        "            self.rewards_records[jobs[1].generate_time] = jobs[1].survival_prob_res\n",
        "  \n",
        "        # add obs to records\n",
        "        if self.tgen_next < 100000:\n",
        "            self.tgen_list.append([self.tgen_next, Legal_Action])\n",
        "            i = 0\n",
        "            j = 0\n",
        "            w_record1 = 0\n",
        "            s_record1 = 0\n",
        "            w_record2 = 0\n",
        "            s_record2 = 0\n",
        "            \n",
        "            for jobs2 in all_served:\n",
        "\n",
        "                # time window -- future consideration\n",
        "                if jobs2[1].departure_time < self.tgen_next and jobs2[1].facility  == 1: # 1hos 2acf\n",
        "                    i += 1\n",
        "                    w_record1 += jobs2[1].waiting_time()            \n",
        "                    s_record1 += jobs2[1].service_time\n",
        "                    \n",
        "                if jobs2[1].departure_time < self.tgen_next and jobs2[1].facility  == 2: # 1hos 2acf\n",
        "                    j += 1\n",
        "                    w_record2 += jobs2[1].waiting_time()            \n",
        "                    s_record2 += jobs2[1].service_time                    \n",
        "\n",
        "            self.wait_records1[self.tgen_next] = 0 if i == 0 else int(round(w_record1/i))   # queuing_time\n",
        "            self.service_records1[self.tgen_next] = 0 if i == 0 else int(round(s_record1/i)) #service_time\n",
        "\n",
        "            self.wait_records2[self.tgen_next] = 0 if j == 0 else int(round(w_record2/j))   # queuing_time\n",
        "            self.service_records2[self.tgen_next] = 0 if j == 0 else int(round(s_record2/j))  #service_time\n",
        "                   \n",
        "            # observation records\n",
        "            self.obs_records[self.tgen_next] = np.array([self.tgen_next, len(self.arrival_list1), len(self.arrival_list2)] \n",
        "                        + self.server1.getSize(self.tgen_next)\n",
        "                        + self.server2.getSize(self.tgen_next)\n",
        "                        + self.server1.getBusyServer(self.tgen_next)\n",
        "                        + self.server2.getBusyServer(self.tgen_next)\n",
        "                        + [self.wait_records1[self.tgen_next],\n",
        "                           self.service_records1[self.tgen_next],\n",
        "                           self.wait_records2[self.tgen_next],\n",
        "                           self.service_records2[self.tgen_next]])\n",
        "                                                        \n",
        "                        # + [len(self.server1.served_jobs)]\n",
        "                        # + [len(self.server2.served_jobs)])\n",
        "            states = {'obs': np.delete(self.obs_records[self.tgen_next],0), \n",
        "                      'legal_actions': Legal_Action}\n",
        "        else:\n",
        "            states = {'obs': self.obs_low,\n",
        "                      'legal_actions': [0,1]}\n",
        "        \n",
        "\n",
        "            \n",
        "             \n",
        "        returns = []\n",
        "\n",
        "        for everyone in self.tgen_list[:]:\n",
        "            try:\n",
        "                rewards = self.rewards_records[everyone[0]]\n",
        "                \n",
        "                obs = self.obs_records[everyone[0]]\n",
        "                \n",
        "                actions = self.action_records[everyone[0]]\n",
        "                \n",
        "                returns.append({'obs': obs,'legal_actions': everyone[1],'action': actions ,'reward': rewards})\n",
        "                self.tgen_list.remove(everyone)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        done = bool(self.tgen_next == 100000)\n",
        "        # obs = np.array([1,1,1,1,1,1,1,1])\n",
        "        # reward = 1\n",
        "        info = {}\n",
        "        \n",
        "        return states, returns, done, info\n",
        "        \n",
        "    def set_agents(self, agents):\n",
        "        ''' Set the agents that will interact with the environment\n",
        "        Args:\n",
        "            agents (list): List of Agent classes\n",
        "        '''\n",
        "        self.agents = agents\n",
        "\n",
        "\n",
        "    def run(self, myj, is_training=False):\n",
        "        '''\n",
        "        Run a complete game, either for evaluation or training RL agent.\n",
        "        Args:\n",
        "            is_training (boolean): True if for training purpose.\n",
        "        Returns:\n",
        "            (tuple) Tuple containing:\n",
        "                (list): A list of trajectories generated from the environment.\n",
        "                (list): A list payoffs. Each entry corresponds to one player.\n",
        "        Note: The trajectories are 3-dimension list. The first dimension is for different players.\n",
        "              The second dimension is for different transitions. The third dimension is for the contents of each transiton\n",
        "        '''\n",
        "        return_summary = []\n",
        "        done = False\n",
        "        \n",
        "        states, returns, done, info = self.reset()\n",
        "        # read arrival data \n",
        "        self.Get_Time(myj)\n",
        "        \n",
        "        while not done:\n",
        "            # action, _states = model.predict(states)\n",
        "            if not is_training:\n",
        "                action, probs_ = self.agents.eval_step(states)\n",
        "                # print(\"best action \", action)\n",
        "                # print(\"probs \", probs_)\n",
        "            else:\n",
        "                action = self.agents.step(states)\n",
        "            \n",
        "            # if random.random() > 0.9:\n",
        "            #     action = 1   \n",
        "            # else:\n",
        "            #     action = 0\n",
        "            # action = 0\n",
        "            \n",
        "            states, returns, done, info = self.step(action)\n",
        "            \n",
        "            return_summary = np.append(return_summary, returns)\n",
        "        \n",
        "        # served_result = self.get_stat()\n",
        "        \n",
        "        # print(served_result)\n",
        "        \n",
        "        list2 = [x for x in return_summary if x != []]\n",
        "        sorted_ts = sorted(list2, key = lambda i: i['obs'][0]) \n",
        "        \n",
        "        trajectories = []\n",
        "\n",
        "        for i in range(len(sorted_ts) - 1):\n",
        "            #np.delete(sorted_ts[i]['obs'],0)\n",
        "            trajectories.append([{'obs': np.delete(sorted_ts[i]['obs'],0), 'legal_actions': sorted_ts[i]['legal_actions']}, sorted_ts[i]['action'],sorted_ts[i]['reward'],\n",
        "          {'obs': np.delete(sorted_ts[i+1]['obs'],0), 'legal_actions': sorted_ts[i+1]['legal_actions']}, bool(i == len(sorted_ts) - 2)])\n",
        "        \n",
        "        return trajectories\n",
        "    \n",
        "    def get_Reward(self):\n",
        "    # Playing Atari with Deep Reinforcement Learning    \n",
        "    # receives a reward rt representing the change in game score.\n",
        "    # survival probablity\n",
        "    # Resource-Based Patient Prioritization in Mass-Casualty Incidents\n",
        "        all_served = self.server1.served_jobs + self.server2.served_jobs\n",
        "        \n",
        "        for jobs in all_served:\n",
        "            # if jobs[1].generate_time not in list(self.rewards_records.keys()):\n",
        "            self.rewards_records[jobs[1].generate_time] = jobs[1].survival_prob_res\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        # return observation\n",
        "        self.tnow = 0\n",
        "        self.gen_patient_time_list = []\n",
        "        self.tgen = 0\n",
        "        self.tnext = 0\n",
        "        self.arrival_list1 = []\n",
        "        self.arrival_list2 = []\n",
        "        self.arrival_time = self.tnow\n",
        "        self.surv_num = 5\n",
        "\n",
        "        \n",
        "        \"\"\"['gen_time', 'num_to_hos', 'num_to_acf', \n",
        "        'ql10' ,'ql11', 'ql20', 'ql21', 'busy_1', 'busy_2','served_1', 'served_2', 'rewards'])\"\"\"    \n",
        "        \"\"\" obs space\"\"\"\n",
        "        \"\"\" ['num_to_hos', 'num_to_acf', 'ql10' ,'ql11', 'ql20', 'ql21', 'busy_1', 'busy_2', 'waiting time ', ' service'])\"\"\"\n",
        "        \n",
        "        # self.observation_space = spaces.Box(self.obs_low, self.obs_high, dtype=np.int32)\n",
        "        \n",
        "        self.tgen_list = []\n",
        "        # pd.DataFrame(columns = ['gen_time', 'num_to_hos', 'num_to_acf', \n",
        "        # 'ql10' ,'ql11', 'ql20', 'ql21', 'busy_1', 'busy_2','served_1', 'served_2', 'rewards'])\n",
        "        self.rewards_records = {}\n",
        "        self.obs_records = {}\n",
        "        self.action_records = {}\n",
        "        self.wait_records1 = {}  # queuing_time\n",
        "        self.service_records1 = {} #service_time\n",
        "        self.wait_records2 = {}  # queuing_time\n",
        "        self.service_records2 = {}\n",
        "        \n",
        "        \n",
        "        self.server1 = Server(12, 'open',self.server1_rate0,self.server1_rate1,self.server1_tt)\n",
        "        self.server2 = Server(6, 'not_open', self.server2_rate0, self.server2_rate1,self.server2_tt)\n",
        "        self.s2_isopen = True\n",
        "        self.simulation_time = 100000\n",
        "        \n",
        "        done = False\n",
        "        info = {}\n",
        "        returns = []\n",
        "        states = {'obs': self.obs_low,'legal_actions': [0,1]}\n",
        "        \n",
        "        return states, returns, done, info\n",
        "\n",
        "    \n",
        "    def get_stat(self):\n",
        "        # Avg_SojournTime, Avg_WaitingTime, Avg_QueueLength\n",
        "        SojournTime1 = 0.0\n",
        "        WaitingTime1 = 0.0\n",
        "        SojournTime2 = 0.0\n",
        "        WaitingTime2 = 0.0\n",
        "        \n",
        "        mydata = []\n",
        "        \n",
        "        for jobs in self.server1.served_jobs:\n",
        "            SojournTime1 += jobs[1].sojourn_time()\n",
        "            WaitingTime1 += jobs[1].waiting_time()\n",
        "            \"\"\" f\"{self.arrival_time}, {self.start_service_time}, {self.service_time}, \n",
        "            {self.departure_time}, {self.facility}, {self.patype} \\n\" \"\"\"\n",
        "\n",
        "            # heappush(mydata, (jobs[1].arrival_time, jobs[1].get_info()))\n",
        "            mydata.append(jobs[1].get_info())\n",
        "            \n",
        "            \"\"\"also attach patient in queue and not release\"\"\"\n",
        "            \n",
        "        for not_complete_job in self.server1.stack:\n",
        "            mydata.append(not_complete_job[1].get_info())\n",
        "            \n",
        "        for not_complete_job in self.server1.queue:\n",
        "            mydata.append(not_complete_job[1].get_info())\n",
        "\n",
        "        with open('Server1.csv', 'w', newline='') as file:\n",
        "            writer = csv.writer(file, delimiter=',')\n",
        "            writer.writerows(mydata)        \n",
        "        \n",
        "        mydata = []\n",
        "        \n",
        "        for jobs in self.server2.served_jobs:\n",
        "            SojournTime2 += jobs[1].sojourn_time()\n",
        "            WaitingTime2 += jobs[1].waiting_time()\n",
        "            mydata.append(jobs[1].get_info())\n",
        "            # mydata.append([jobs[1].arrival_time,jobs[1].start_service_time,jobs[1].service_time, jobs[1].departure_time])\n",
        "            # print(\"{0:.8f}\".format(jobs[1].arrival_time), \"{0:.8f}\".format(jobs[1].start_service_time),\\\n",
        "            #       \"{0:.8f}\".format(jobs[1].service_time), \"{0:.8f}\".format(jobs[1].departure_time))\n",
        "            # print(jobs[1].get_info())\n",
        "            \n",
        "        for not_complete_job in self.server2.stack:\n",
        "            mydata.append(not_complete_job[1].get_info())\n",
        "            \n",
        "        for not_complete_job in self.server2.queue:\n",
        "            mydata.append(not_complete_job[1].get_info())            \n",
        "        \n",
        "        with open('Server2.csv', 'w', newline='') as file:\n",
        "            writer = csv.writer(file, delimiter=',')\n",
        "            writer.writerows(mydata) \n",
        "            \n",
        "            \n",
        "        if len(self.server1.served_jobs) != 0:\n",
        "            Avg_SojournTime1 = SojournTime1/len(self.server1.served_jobs)\n",
        "            Avg_WaitingTime1 = WaitingTime1/len(self.server1.served_jobs)\n",
        "        else:    \n",
        "            Avg_SojournTime1 = None\n",
        "            Avg_WaitingTime1 = None\n",
        "\n",
        "        if len(self.server2.served_jobs) != 0:\n",
        "            Avg_SojournTime2 = SojournTime2/len(self.server2.served_jobs)             \n",
        "            Avg_WaitingTime2 = WaitingTime2/len(self.server2.served_jobs)\n",
        "        else:\n",
        "            Avg_SojournTime2 = None\n",
        "            Avg_WaitingTime2 = None\n",
        "            \n",
        "        \n",
        "        return  Avg_SojournTime1, Avg_WaitingTime1, \\\n",
        "            Avg_SojournTime2, Avg_WaitingTime2\n",
        "                # Avg_QueueLength1, Avg_QueueLength2\n",
        "        \n",
        "    def debug(self):\n",
        "        print(\"==============\")\n",
        "        # self.server1.queue,\n",
        "        # print(\"server1  \", self.server1.c, self.server1.action, \\\n",
        "              # self.server1.getSize(),  self.server1.stack)\n",
        "        # print(\"served job \", self.server1.served_jobs)\n",
        "        # print(\"server2  \", self.server2.c, self.server2.action, \\\n",
        "        # self.server2.getSize(), self.server2.stack)\n",
        "\n",
        "\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AkpLa7UxOYj"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pandas as pd\n",
        "import rlcard\n",
        "\n",
        "from rlcard.utils import set_global_seed\n",
        "from rlcard.utils import Logger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au65qNEZ1MMZ",
        "cellView": "form"
      },
      "source": [
        "#@title Code for Agent\n",
        "''' DQN agent\n",
        "The code is derived from https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/dqn.py\n",
        "Copyright (c) 2019 DATA Lab at Texas A&M University\n",
        "Copyright (c) 2016 Denny Britz\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "'''\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import namedtuple\n",
        "\n",
        "from rlcard.utils.utils import remove_illegal\n",
        "\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "\n",
        "class DQNAgent(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 sess,\n",
        "                 scope,\n",
        "                 replay_memory_size=10000,\n",
        "                 replay_memory_init_size=200,\n",
        "                 update_target_estimator_every=1000,\n",
        "                 discount_factor=0.99,\n",
        "                 epsilon_start=1.0,\n",
        "                 epsilon_end=0.1,\n",
        "                 epsilon_decay_steps=10000,\n",
        "                 batch_size=32,\n",
        "                 action_num=2,\n",
        "                 state_shape=None,\n",
        "                 train_every=1,\n",
        "                 mlp_layers=None,\n",
        "                 learning_rate=0.00005):\n",
        "\n",
        "        '''\n",
        "        Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
        "        Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
        "        Args:\n",
        "            sess (tf.Session): Tensorflow Session object.\n",
        "            scope (string): The name scope of the DQN agent.\n",
        "            replay_memory_size (int): Size of the replay memory\n",
        "            replay_memory_init_size (int): Number of random experiences to sampel when initializing\n",
        "              the reply memory.\n",
        "            train_every (int): Train the agent every X steps.\n",
        "            update_target_estimator_every (int): Copy parameters from the Q estimator to the\n",
        "              target estimator every N steps\n",
        "            discount_factor (float): Gamma discount factor\n",
        "            epsilon_start (int): Chance to sample a random action when taking an action.\n",
        "              Epsilon is decayed over time and this is the start value\n",
        "            epsilon_end (int): The final minimum value of epsilon after decaying is done\n",
        "            epsilon_decay_steps (int): Number of steps to decay epsilon over\n",
        "            batch_size (int): Size of batches to sample from the replay memory\n",
        "            evaluate_every (int): Evaluate every N steps\n",
        "            action_num (int): The number of the actions\n",
        "            state_space (list): The space of the state vector\n",
        "            train_every (int): Train the network every X steps.\n",
        "            mlp_layers (list): The layer number and the dimension of each layer in MLP\n",
        "            learning_rate (float): The learning rate of the DQN agent.\n",
        "        '''\n",
        "        self.use_raw = False\n",
        "        self.sess = sess\n",
        "        self.scope = scope\n",
        "        self.replay_memory_init_size = replay_memory_init_size\n",
        "        self.update_target_estimator_every = update_target_estimator_every\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon_decay_steps = epsilon_decay_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.action_num = action_num\n",
        "        self.train_every = train_every\n",
        "\n",
        "        # Total timesteps\n",
        "        self.total_t = 0\n",
        "\n",
        "        # Total training step\n",
        "        self.train_t = 0\n",
        "\n",
        "        # The epsilon decay scheduler\n",
        "        self.epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
        "\n",
        "        # Create estimators\n",
        "        self.q_estimator = Estimator(scope=self.scope+\"_q\", action_num=action_num, learning_rate=learning_rate, state_shape=state_shape, mlp_layers=mlp_layers)\n",
        "        self.target_estimator = Estimator(scope=self.scope+\"_target_q\", action_num=action_num, learning_rate=learning_rate, state_shape=state_shape, mlp_layers=mlp_layers)\n",
        "\n",
        "        # Create replay memory\n",
        "        self.memory = Memory(replay_memory_size, batch_size)\n",
        "\n",
        "    def feed(self, ts):\n",
        "        ''' Store data in to replay buffer and train the agent. There are two stages.\n",
        "            In stage 1, populate the memory without training\n",
        "            In stage 2, train the agent every several timesteps\n",
        "        Args:\n",
        "            ts (list): a list of 5 elements that represent the transition\n",
        "        '''\n",
        "        (state, action, reward, next_state, done) = tuple(ts)\n",
        "        self.feed_memory(state['obs'], action, reward, next_state['obs'], done)\n",
        "        self.total_t += 1\n",
        "        tmp = self.total_t - self.replay_memory_init_size\n",
        "        if tmp>=0 and tmp%self.train_every == 0:\n",
        "            self.train()\n",
        "\n",
        "    def step(self, state):\n",
        "        ''' Predict the action for generating training data\n",
        "        Args:\n",
        "            state (numpy.array): current state\n",
        "        Returns:\n",
        "            action (int): an action id\n",
        "        '''\n",
        "        A = self.predict(state['obs'])\n",
        "        A = remove_illegal(A, state['legal_actions'])\n",
        "        action = np.random.choice(np.arange(len(A)), p=A)\n",
        "        return action\n",
        "\n",
        "    def eval_step(self, state):\n",
        "        ''' Predict the action for evaluation purpose.\n",
        "        Args:\n",
        "            state (numpy.array): current state\n",
        "        Returns:\n",
        "            action (int): an action id\n",
        "            probs (list): a list of probabilies\n",
        "        '''\n",
        "        q_values = self.q_estimator.predict(self.sess, np.expand_dims(state['obs'], 0))[0]\n",
        "        probs = remove_illegal(np.exp(q_values), state['legal_actions'])\n",
        "        best_action = np.argmax(probs)\n",
        "\n",
        "        # print(\"q_values \", q_values)\n",
        "        # print(\"eval_step \", state, best_action, probs)\n",
        "        return best_action, probs\n",
        "\n",
        "    def predict(self, state):\n",
        "        ''' Predict the action probabilities\n",
        "        Args:\n",
        "            state (numpy.array): current state\n",
        "        Returns:\n",
        "            q_values (numpy.array): a 1-d array where each entry represents a Q value\n",
        "        '''\n",
        "        epsilon = self.epsilons[min(self.total_t, self.epsilon_decay_steps-1)]\n",
        "        A = np.ones(self.action_num, dtype=float) * epsilon / self.action_num\n",
        "        q_values = self.q_estimator.predict(self.sess, np.expand_dims(state, 0))[0]\n",
        "        best_action = np.argmax(q_values)\n",
        "        A[best_action] += (1.0 - epsilon)\n",
        "        return A\n",
        "\n",
        "    def train(self):\n",
        "        ''' Train the network\n",
        "        Returns:\n",
        "            loss (float): The loss of the current batch.\n",
        "        '''\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample()\n",
        "        # Calculate q values and targets (Double DQN)\n",
        "        q_values_next = self.q_estimator.predict(self.sess, next_state_batch)\n",
        "        best_actions = np.argmax(q_values_next, axis=1)\n",
        "        q_values_next_target = self.target_estimator.predict(self.sess, next_state_batch)\n",
        "        target_batch = reward_batch + np.invert(done_batch).astype(np.float32) * \\\n",
        "            self.discount_factor * q_values_next_target[np.arange(self.batch_size), best_actions]\n",
        "\n",
        "        # Perform gradient descent update\n",
        "        state_batch = np.array(state_batch)\n",
        "        loss = self.q_estimator.update(self.sess, state_batch, action_batch, target_batch)\n",
        "        print('\\rINFO - Agent {}, step {}, rl-loss: {}'.format(self.scope, self.total_t, loss), end='')\n",
        "\n",
        "\n",
        "        # Update the target estimator\n",
        "        if self.train_t % self.update_target_estimator_every == 0:\n",
        "            copy_model_parameters(self.sess, self.q_estimator, self.target_estimator)\n",
        "            print(\"\\nINFO - Copied model parameters to target network.\")\n",
        "\n",
        "        self.train_t += 1\n",
        "\n",
        "    def feed_memory(self, state, action, reward, next_state, done):\n",
        "        ''' Feed transition to memory\n",
        "        Args:\n",
        "            state (numpy.array): the current state\n",
        "            action (int): the performed action ID\n",
        "            reward (float): the reward received\n",
        "            next_state (numpy.array): the next state after performing the action\n",
        "            done (boolean): whether the episode is finished\n",
        "        '''\n",
        "        self.memory.save(state, action, reward, next_state, done)\n",
        "\n",
        "    def copy_params_op(self, global_vars):\n",
        "        ''' Copys the variables of two estimator to others.\n",
        "        Args:\n",
        "            global_vars (list): A list of tensor\n",
        "        '''\n",
        "        self_vars = tf.contrib.slim.get_variables(scope=self.scope, collection=tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "        update_ops = []\n",
        "        for v1, v2 in zip(global_vars, self_vars):\n",
        "            op = v2.assign(v1)\n",
        "            update_ops.append(op)\n",
        "        self.sess.run(update_ops)\n",
        "\n",
        "class Estimator():\n",
        "    ''' Q-Value Estimator neural network.\n",
        "        This network is used for both the Q-Network and the Target Network.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, scope=\"estimator\", action_num=2, learning_rate=0.001, state_shape=None, mlp_layers=None):\n",
        "        ''' Initilalize an Estimator object.\n",
        "        Args:\n",
        "            action_num (int): the number output actions\n",
        "            state_shap (list): the shape of the state space\n",
        "        '''\n",
        "        self.scope = scope\n",
        "        self.action_num = action_num\n",
        "        self.learning_rate=learning_rate\n",
        "        self.state_shape = state_shape if isinstance(state_shape, list) else [state_shape]\n",
        "        self.mlp_layers = map(int, mlp_layers)\n",
        "\n",
        "        with tf.variable_scope(scope):\n",
        "            # Build the graph\n",
        "            self._build_model()\n",
        "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=tf.get_variable_scope().name)\n",
        "        # Optimizer Parameters from original paper\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, name='dqn_adam')\n",
        "\n",
        "        with tf.control_dependencies(update_ops):\n",
        "            self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
        "\n",
        "    def _build_model(self):\n",
        "        ''' Build an MLP model.\n",
        "        '''\n",
        "        # Placeholders for our input\n",
        "        # Our input are 4 RGB frames of shape 160, 160 each\n",
        "        input_shape = [None]\n",
        "        input_shape.extend(self.state_shape)\n",
        "        self.X_pl = tf.placeholder(shape=input_shape, dtype=tf.float32, name=\"X\")\n",
        "        # The TD target value\n",
        "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
        "        # Integer id of which action was selected\n",
        "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
        "        # Boolean to indicate whether is training or not\n",
        "        self.is_train = tf.placeholder(tf.bool, name=\"is_train\")\n",
        "\n",
        "        batch_size = tf.shape(self.X_pl)[0]\n",
        "\n",
        "        # Batch Normalization\n",
        "        X = tf.layers.batch_normalization(self.X_pl, training=self.is_train)\n",
        "\n",
        "        # Fully connected layers\n",
        "        fc = tf.contrib.layers.flatten(X)\n",
        "        for dim in self.mlp_layers:\n",
        "            fc = tf.contrib.layers.fully_connected(fc, dim, activation_fn=tf.tanh)\n",
        "        self.predictions = tf.contrib.layers.fully_connected(fc, self.action_num, activation_fn=None)\n",
        "\n",
        "        # Get the predictions for the chosen actions only\n",
        "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
        "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
        "\n",
        "        # Calculate the loss\n",
        "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
        "        self.loss = tf.reduce_mean(self.losses)\n",
        "\n",
        "    def predict(self, sess, s):\n",
        "        ''' Predicts action values.\n",
        "        Args:\n",
        "          sess (tf.Session): Tensorflow Session object\n",
        "          s (numpy.array): State input of shape [batch_size, 4, 160, 160, 3]\n",
        "          is_train (boolean): True if is training\n",
        "        Returns:\n",
        "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated\n",
        "          action values.\n",
        "        '''\n",
        "        return sess.run(self.predictions, { self.X_pl: s, self.is_train:False})\n",
        "\n",
        "    def update(self, sess, s, a, y):\n",
        "        ''' Updates the estimator towards the given targets.\n",
        "        Args:\n",
        "          sess (tf.Session): Tensorflow Session object\n",
        "          s (list): State input of shape [batch_size, 4, 160, 160, 3]\n",
        "          a (list): Chosen actions of shape [batch_size]\n",
        "          y (list): Targets of shape [batch_size]\n",
        "        Returns:\n",
        "          The calculated loss on the batch.\n",
        "        '''\n",
        "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a, self.is_train: True}\n",
        "        _, _, loss = sess.run(\n",
        "                [tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
        "                feed_dict)\n",
        "        return loss\n",
        "\n",
        "class Memory(object):\n",
        "    ''' Memory for saving transitions\n",
        "    '''\n",
        "\n",
        "    def __init__(self, memory_size, batch_size):\n",
        "        ''' Initialize\n",
        "        Args:\n",
        "            memory_size (int): the size of the memroy buffer\n",
        "        '''\n",
        "        self.memory_size = memory_size\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = []\n",
        "\n",
        "    def save(self, state, action, reward, next_state, done):\n",
        "        ''' Save transition into memory\n",
        "        Args:\n",
        "            state (numpy.array): the current state\n",
        "            action (int): the performed action ID\n",
        "            reward (float): the reward received\n",
        "            next_state (numpy.array): the next state after performing the action\n",
        "            done (boolean): whether the episode is finished\n",
        "        '''\n",
        "        if len(self.memory) == self.memory_size:\n",
        "            self.memory.pop(0)\n",
        "        transition = Transition(state, action, reward, next_state, done)\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self):\n",
        "        ''' Sample a minibatch from the replay memory\n",
        "        Returns:\n",
        "            state_batch (list): a batch of states\n",
        "            action_batch (list): a batch of actions\n",
        "            reward_batch (list): a batch of rewards\n",
        "            next_state_batch (list): a batch of states\n",
        "            done_batch (list): a batch of dones\n",
        "        '''\n",
        "        samples = random.sample(self.memory, self.batch_size)\n",
        "        return map(np.array, zip(*samples))\n",
        "\n",
        "def copy_model_parameters(sess, estimator1, estimator2):\n",
        "    ''' Copys the model parameters of one estimator to another.\n",
        "    Args:\n",
        "        sess (tf.Session): Tensorflow Session object\n",
        "        estimator1 (Estimator): Estimator to copy the paramters from\n",
        "        estimator2 (Estimator): Estimator to copy the parameters to\n",
        "    '''\n",
        "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
        "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
        "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
        "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
        "\n",
        "    update_ops = []\n",
        "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
        "        op = e2_v.assign(e1_v)\n",
        "        update_ops.append(op)\n",
        "\n",
        "    sess.run(update_ops)\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "#    with tf.Session() as sess:\n",
        "#        agent = DQNAgent(sess,\n",
        "#                         scope='dqn',\n",
        "#                         action_num=4,\n",
        "#                         replay_memory_init_size=100,\n",
        "#                         norm_step=100,\n",
        "#                         state_shape=[2],\n",
        "#                         mlp_layers=[10,10])\n",
        "#\n",
        "#        for a in tf.global_variables():\n",
        "#            print(a)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdXItFuWahIb"
      },
      "source": [
        "# Make environment\n",
        "env = Env(0)\n",
        "eval_env = Env(0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WagAZhdAYcmG"
      },
      "source": [
        "# Set the iterations numbers and how frequently we evaluate/save plot\n",
        "evaluate_every = 200\n",
        "evaluate_num = 1000\n",
        "episode_num = 10000\n",
        "\n",
        "# The intial memory size\n",
        "memory_init_size = 200\n",
        "\n",
        "# Train the agent every X steps\n",
        "train_every = 1\n",
        "\n",
        "# The paths for saving the logs and learning curves\n",
        "log_dir = './experiments/blackjack_dqn_result/'\n",
        "\n",
        "# Set a global seed\n",
        "tf.compat.v1.set_random_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2W2O1hOaL6P",
        "outputId": "0e95948c-dbcd-493c-8ea9-bc7b24aa92f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 21304
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "\n",
        "    # Initialize a global step\n",
        "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "\n",
        "    # Set up the agents\n",
        "    agent = DQNAgent(sess,\n",
        "                     scope='dqn',\n",
        "                     action_num=env.action_num,\n",
        "                     replay_memory_init_size=memory_init_size,\n",
        "                     train_every=train_every,\n",
        "                     state_shape=[12],\n",
        "                     mlp_layers=[2,2])\n",
        "    env.set_agents(agent)\n",
        "    eval_env.set_agents(agent)\n",
        "\n",
        "    # Initialize global variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # Init a Logger to plot the learning curve\n",
        "    logger = Logger(log_dir)\n",
        "\n",
        "    for episode in range(episode_num):\n",
        "        # print('\\n episode1 ', episode)\n",
        "        # Generate data from the environment\n",
        "        trajectories = env.run(myj = 1, is_training=True)\n",
        "        # print(\"trajectories \", trajectories)\n",
        "\n",
        "        # Feed transitions into agent memory, and train the agent\n",
        "        for ts in trajectories:\n",
        "            # print(\"ts \", ts)\n",
        "            agent.feed(ts)\n",
        "\n",
        "        # Evaluate the performance. Play with random agents.\n",
        "        #if episode % evaluate_every == 0:\n",
        "        #    logger.log_performance(env.timestep, tournament(eval_env, evaluate_num)[0])\n",
        "\n",
        "        # Evaluate the performance. Play with random agents.\n",
        "        if episode % evaluate_every == 0:\n",
        "          # print('episode2 ', episode)\n",
        "          # print(\"logger \", eva_perform(eval_env, evaluate_num))\n",
        "          logger.log_performance(episode, eva_perform(eval_env, evaluate_num))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-7c47c2bddd02>:253: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-8-7c47c2bddd02>:233: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_global_step\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  0\n",
            "  reward       |  24.091719840626077\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 200, rl-loss: 0.9000279903411865\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 1200, rl-loss: 0.42293697595596313\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 2200, rl-loss: 0.3881060779094696\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 3200, rl-loss: 0.2160922735929489\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 4200, rl-loss: 0.19079571962356567\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 5200, rl-loss: 0.2109437882900238\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 6200, rl-loss: 0.11476550996303558\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 7200, rl-loss: 0.10047610104084015\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 8200, rl-loss: 0.12028522789478302\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 9200, rl-loss: 0.08812304586172104\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 9849, rl-loss: 0.17212367057800293\n",
            "----------------------------------------\n",
            "  timestep     |  200\n",
            "  reward       |  26.324503261960448\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 10200, rl-loss: 0.14907819032669067\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 11200, rl-loss: 0.20923127233982086\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 12200, rl-loss: 0.07402205467224121\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 13200, rl-loss: 0.07973145693540573\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 14200, rl-loss: 0.1726939082145691\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 15200, rl-loss: 0.08070435374975204\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 16200, rl-loss: 0.132491797208786\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 17200, rl-loss: 0.06313224136829376\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 18200, rl-loss: 0.07085418701171875\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 19200, rl-loss: 0.09254960715770721\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 19649, rl-loss: 0.19976654648780823\n",
            "----------------------------------------\n",
            "  timestep     |  400\n",
            "  reward       |  26.49686836286228\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 20200, rl-loss: 0.12483258545398712\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 21200, rl-loss: 0.19834285974502563\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 22200, rl-loss: 0.05714557692408562\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 23200, rl-loss: 0.14076465368270874\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 24200, rl-loss: 0.217422217130661\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 25200, rl-loss: 0.08288939297199249\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 26200, rl-loss: 0.13822725415229797\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 27200, rl-loss: 0.12419053912162781\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 28200, rl-loss: 0.11625994741916656\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 29200, rl-loss: 0.09373323619365692\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 29449, rl-loss: 0.10947516560554504\n",
            "----------------------------------------\n",
            "  timestep     |  600\n",
            "  reward       |  26.511059089094797\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 30200, rl-loss: 0.08122801780700684\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 31200, rl-loss: 0.07880587875843048\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 32200, rl-loss: 0.08283787220716476\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 33200, rl-loss: 0.15651726722717285\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 34200, rl-loss: 0.06304573267698288\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 35200, rl-loss: 0.050827622413635254\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 36200, rl-loss: 0.05135402828454971\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 37200, rl-loss: 0.07088802754878998\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 38200, rl-loss: 0.07948989421129227\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 39200, rl-loss: 0.12686142325401306\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 39249, rl-loss: 0.041715625673532486\n",
            "----------------------------------------\n",
            "  timestep     |  800\n",
            "  reward       |  26.474010601431296\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 40200, rl-loss: 0.11517762392759323\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 41200, rl-loss: 0.04037057235836983\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 42200, rl-loss: 0.06634441018104553\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 43200, rl-loss: 0.05645273998379707\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 44200, rl-loss: 0.028260067105293274\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 45200, rl-loss: 0.025647345930337906\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 46200, rl-loss: 0.02435343526303768\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 47200, rl-loss: 0.04227503761649132\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 48200, rl-loss: 0.039633505046367645\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 49049, rl-loss: 0.02651739865541458\n",
            "----------------------------------------\n",
            "  timestep     |  1000\n",
            "  reward       |  26.484649633605226\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 49200, rl-loss: 0.04451509565114975\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 50200, rl-loss: 0.030593503266572952\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 51200, rl-loss: 0.03858210891485214\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 52200, rl-loss: 0.01468752883374691\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 53200, rl-loss: 0.09706110507249832\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 54200, rl-loss: 0.027181748300790787\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 55200, rl-loss: 0.04244087636470795\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 56200, rl-loss: 0.016993064433336258\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 57200, rl-loss: 0.0939445048570633\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 58200, rl-loss: 0.03828844055533409\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 58849, rl-loss: 0.08717109262943268\n",
            "----------------------------------------\n",
            "  timestep     |  1200\n",
            "  reward       |  26.480424160290305\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 59200, rl-loss: 0.0558600053191185\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 60200, rl-loss: 0.044384509325027466\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 61200, rl-loss: 0.017371632158756256\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 62200, rl-loss: 0.07477228343486786\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 63200, rl-loss: 0.017534492537379265\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 64200, rl-loss: 0.018375828862190247\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 65200, rl-loss: 0.09396041929721832\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 66200, rl-loss: 0.08200481534004211\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 67200, rl-loss: 0.11215745657682419\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 68200, rl-loss: 0.10285087674856186\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 68649, rl-loss: 0.06895070523023605\n",
            "----------------------------------------\n",
            "  timestep     |  1400\n",
            "  reward       |  26.427486392901553\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 69200, rl-loss: 0.025626882910728455\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 70200, rl-loss: 0.03762516751885414\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 71200, rl-loss: 0.05450624227523804\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 72200, rl-loss: 0.07329341769218445\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 73200, rl-loss: 0.062293075025081635\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 74200, rl-loss: 0.02509158104658127\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 75200, rl-loss: 0.04295314848423004\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 76200, rl-loss: 0.015022588893771172\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 77200, rl-loss: 0.02358248643577099\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 78200, rl-loss: 0.05542386323213577\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 78449, rl-loss: 0.046436525881290436\n",
            "----------------------------------------\n",
            "  timestep     |  1600\n",
            "  reward       |  26.58006042822683\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 79200, rl-loss: 0.014474151656031609\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 80200, rl-loss: 0.01970692165195942\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 81200, rl-loss: 0.026005972176790237\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 82200, rl-loss: 0.040820103138685226\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 83200, rl-loss: 0.11124580353498459\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 84200, rl-loss: 0.01216902770102024\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 85200, rl-loss: 0.07547964155673981\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 86200, rl-loss: 0.07142733037471771\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 87200, rl-loss: 0.02901502698659897\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 88200, rl-loss: 0.10791338235139847\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 88249, rl-loss: 0.0475793294608593\n",
            "----------------------------------------\n",
            "  timestep     |  1800\n",
            "  reward       |  26.39930818765074\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 89200, rl-loss: 0.05298655852675438\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 90200, rl-loss: 0.007373863831162453\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 91200, rl-loss: 0.07962513715028763\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 92200, rl-loss: 0.0194830521941185\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 93200, rl-loss: 0.022275784984230995\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 94200, rl-loss: 0.067533940076828\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 95200, rl-loss: 0.018798472359776497\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 96200, rl-loss: 0.04012690484523773\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 97200, rl-loss: 0.0962139368057251\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 98049, rl-loss: 0.09646604210138321\n",
            "----------------------------------------\n",
            "  timestep     |  2000\n",
            "  reward       |  26.523984606773112\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 98200, rl-loss: 0.032899484038352966\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 99200, rl-loss: 0.019695017486810684\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 100200, rl-loss: 0.013097318820655346\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 101200, rl-loss: 0.009264347143471241\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 102200, rl-loss: 0.02563275210559368\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 103200, rl-loss: 0.024106446653604507\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 104200, rl-loss: 0.012954363599419594\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 105200, rl-loss: 0.05919475480914116\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 106200, rl-loss: 0.03784123435616493\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 107200, rl-loss: 0.06450752913951874\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 107849, rl-loss: 0.06359470635652542\n",
            "----------------------------------------\n",
            "  timestep     |  2200\n",
            "  reward       |  26.487809625926623\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 108200, rl-loss: 0.05260232836008072\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 109200, rl-loss: 0.08619356900453568\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 110200, rl-loss: 0.043335314840078354\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 111200, rl-loss: 0.016259904950857162\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 112200, rl-loss: 0.01456474605947733\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 113200, rl-loss: 0.022141214460134506\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 114200, rl-loss: 0.06514181941747665\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 115200, rl-loss: 0.019804710522294044\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 116200, rl-loss: 0.018240487203001976\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 117200, rl-loss: 0.05925842002034187\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 117649, rl-loss: 0.013033390045166016\n",
            "----------------------------------------\n",
            "  timestep     |  2400\n",
            "  reward       |  26.443926908510154\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 118200, rl-loss: 0.018574098125100136\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 119200, rl-loss: 0.057170018553733826\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 120200, rl-loss: 0.07676787674427032\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 121200, rl-loss: 0.011362995952367783\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 122200, rl-loss: 0.009719479829072952\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 123200, rl-loss: 0.0138937309384346\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 124200, rl-loss: 0.06249832361936569\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 125200, rl-loss: 0.07345756143331528\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 126200, rl-loss: 0.028155067935585976\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 127200, rl-loss: 0.018917106091976166\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 127449, rl-loss: 0.012529483065009117\n",
            "----------------------------------------\n",
            "  timestep     |  2600\n",
            "  reward       |  26.4706776073625\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 128200, rl-loss: 0.020452436059713364\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 129200, rl-loss: 0.05685596540570259\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 130200, rl-loss: 0.014588873833417892\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 131200, rl-loss: 0.13547383248806\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 132200, rl-loss: 0.04347574710845947\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 133200, rl-loss: 0.07442831248044968\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 134200, rl-loss: 0.014329932630062103\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 135200, rl-loss: 0.07778885215520859\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 136200, rl-loss: 0.06704473495483398\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 137200, rl-loss: 0.08270782232284546\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 137249, rl-loss: 0.013008398935198784\n",
            "----------------------------------------\n",
            "  timestep     |  2800\n",
            "  reward       |  26.456036221167885\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 138200, rl-loss: 0.016600077971816063\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 139200, rl-loss: 0.08325803279876709\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 140200, rl-loss: 0.07096824795007706\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 141200, rl-loss: 0.06657246500253677\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 142200, rl-loss: 0.07576829195022583\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 143200, rl-loss: 0.016651367768645287\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 144200, rl-loss: 0.03909991681575775\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 145200, rl-loss: 0.11432558298110962\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 146200, rl-loss: 0.029174858704209328\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 147049, rl-loss: 0.07518505305051804\n",
            "----------------------------------------\n",
            "  timestep     |  3000\n",
            "  reward       |  26.509640945379545\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 147200, rl-loss: 0.0747307613492012\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 148200, rl-loss: 0.0529160276055336\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 149200, rl-loss: 0.11395899951457977\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 150200, rl-loss: 0.018048031255602837\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 151200, rl-loss: 0.15862786769866943\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 152200, rl-loss: 0.07616697996854782\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 153200, rl-loss: 0.13568155467510223\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 154200, rl-loss: 0.1523003876209259\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 155200, rl-loss: 0.01437156181782484\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 156200, rl-loss: 0.019094428047537804\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 156849, rl-loss: 0.02323492430150509\n",
            "----------------------------------------\n",
            "  timestep     |  3200\n",
            "  reward       |  26.5262807091457\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 157200, rl-loss: 0.018642013892531395\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 158200, rl-loss: 0.12843628227710724\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 159200, rl-loss: 0.15569853782653809\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 160200, rl-loss: 0.05260153114795685\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 161200, rl-loss: 0.013066235929727554\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 162200, rl-loss: 0.03566521778702736\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 163200, rl-loss: 0.05375796556472778\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 164200, rl-loss: 0.026763226836919785\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 165200, rl-loss: 0.14129261672496796\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 166200, rl-loss: 0.08285282552242279\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 166649, rl-loss: 0.15649934113025665\n",
            "----------------------------------------\n",
            "  timestep     |  3400\n",
            "  reward       |  26.45662224593929\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 167200, rl-loss: 0.017406273633241653\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 168200, rl-loss: 0.030118240043520927\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 169200, rl-loss: 0.02314952202141285\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 170200, rl-loss: 0.08186885714530945\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 171200, rl-loss: 0.10599346458911896\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 172200, rl-loss: 0.12542887032032013\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 173200, rl-loss: 0.021751143038272858\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 174200, rl-loss: 0.027958236634731293\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 175200, rl-loss: 0.19618535041809082\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 176200, rl-loss: 0.08392433822154999\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 176449, rl-loss: 0.09347929060459137\n",
            "----------------------------------------\n",
            "  timestep     |  3600\n",
            "  reward       |  26.46661306319693\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 177200, rl-loss: 0.02132701687514782\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 178200, rl-loss: 0.013899371027946472\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 179200, rl-loss: 0.10007812082767487\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 180200, rl-loss: 0.017059918493032455\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 181200, rl-loss: 0.07770252972841263\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 182200, rl-loss: 0.08436812460422516\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 183200, rl-loss: 0.03406200930476189\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 184200, rl-loss: 0.007876291871070862\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 185200, rl-loss: 0.083111472427845\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 186200, rl-loss: 0.19420555233955383\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 186249, rl-loss: 0.020611297339200974\n",
            "----------------------------------------\n",
            "  timestep     |  3800\n",
            "  reward       |  26.489992117160906\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 187200, rl-loss: 0.04367392510175705\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 188200, rl-loss: 0.07383998483419418\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 189200, rl-loss: 0.010190701112151146\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 190200, rl-loss: 0.25978174805641174\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 191200, rl-loss: 0.049727603793144226\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 192200, rl-loss: 0.01419888250529766\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 193200, rl-loss: 0.11994148790836334\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 194200, rl-loss: 0.050075750797986984\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 195200, rl-loss: 0.02050960063934326\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 196049, rl-loss: 0.18858113884925842\n",
            "----------------------------------------\n",
            "  timestep     |  4000\n",
            "  reward       |  26.485575575170397\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 196200, rl-loss: 0.12470605969429016\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 197200, rl-loss: 0.07047690451145172\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 198200, rl-loss: 0.013707539066672325\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 199200, rl-loss: 0.05112890154123306\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 200200, rl-loss: 0.02224070206284523\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 201200, rl-loss: 0.01757919043302536\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 202200, rl-loss: 0.012556628324091434\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 203200, rl-loss: 0.05216309428215027\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 204200, rl-loss: 0.249550461769104\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 205200, rl-loss: 0.16256728768348694\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 205849, rl-loss: 0.10388895869255066\n",
            "----------------------------------------\n",
            "  timestep     |  4200\n",
            "  reward       |  26.47518764335288\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 206200, rl-loss: 0.15264007449150085\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 207200, rl-loss: 0.026138335466384888\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 208200, rl-loss: 0.013460792601108551\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 209200, rl-loss: 0.038364067673683167\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 210200, rl-loss: 0.08580445498228073\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 211200, rl-loss: 0.01710120588541031\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 212200, rl-loss: 0.10701850801706314\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 213200, rl-loss: 0.09166219830513\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 214200, rl-loss: 0.03032456897199154\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 215200, rl-loss: 0.1254432052373886\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 215649, rl-loss: 0.13879910111427307\n",
            "----------------------------------------\n",
            "  timestep     |  4400\n",
            "  reward       |  26.49409429897346\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 216200, rl-loss: 0.11196798831224442\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 217200, rl-loss: 0.011149995028972626\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 218200, rl-loss: 0.014704430475831032\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 219200, rl-loss: 0.1020011156797409\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 220200, rl-loss: 0.017666177824139595\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 221200, rl-loss: 0.10655294358730316\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 222200, rl-loss: 0.04594703018665314\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 223200, rl-loss: 0.018253657966852188\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 224200, rl-loss: 0.02379767596721649\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 225200, rl-loss: 0.015118003822863102\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 225449, rl-loss: 0.024498911574482918\n",
            "----------------------------------------\n",
            "  timestep     |  4600\n",
            "  reward       |  26.460790165881473\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 226200, rl-loss: 0.1701337844133377\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 227200, rl-loss: 0.183889240026474\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 228200, rl-loss: 0.11033323407173157\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 229200, rl-loss: 0.03501748666167259\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 230200, rl-loss: 0.02068731002509594\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 231200, rl-loss: 0.022526856511831284\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 232200, rl-loss: 0.01751994714140892\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 233200, rl-loss: 0.09197144210338593\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 234200, rl-loss: 0.20844142138957977\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 235200, rl-loss: 0.014201320707798004\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 235249, rl-loss: 0.01127255242317915\n",
            "----------------------------------------\n",
            "  timestep     |  4800\n",
            "  reward       |  26.482544725064937\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 236200, rl-loss: 0.08531474322080612\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 237200, rl-loss: 0.12076812237501144\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 238200, rl-loss: 0.011562179774045944\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 239200, rl-loss: 0.02445533126592636\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 240200, rl-loss: 0.013365388847887516\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 241200, rl-loss: 0.13200846314430237\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 242200, rl-loss: 0.05239766463637352\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 243200, rl-loss: 0.13487733900547028\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 244200, rl-loss: 0.10964297503232956\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 245049, rl-loss: 0.1154237613081932\n",
            "----------------------------------------\n",
            "  timestep     |  5000\n",
            "  reward       |  26.523788153053527\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 245200, rl-loss: 0.11828344315290451\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 246200, rl-loss: 0.013380700722336769\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 247200, rl-loss: 0.1189182847738266\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 248200, rl-loss: 0.05759692192077637\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 249200, rl-loss: 0.01830509677529335\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 250200, rl-loss: 0.011310987174510956\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 251200, rl-loss: 0.12295632064342499\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 252200, rl-loss: 0.02147570252418518\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 253200, rl-loss: 0.09934552013874054\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 254200, rl-loss: 0.014913525432348251\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 254849, rl-loss: 0.1150221973657608\n",
            "----------------------------------------\n",
            "  timestep     |  5200\n",
            "  reward       |  26.497278674832764\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 255200, rl-loss: 0.022443821653723717\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 256200, rl-loss: 0.1108078733086586\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 257200, rl-loss: 0.0987609401345253\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 258200, rl-loss: 0.018728027120232582\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 259200, rl-loss: 0.016115780919790268\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 260200, rl-loss: 0.2079261839389801\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 261200, rl-loss: 0.017568856477737427\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 262200, rl-loss: 0.013391217216849327\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 263200, rl-loss: 0.020540636032819748\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 264200, rl-loss: 0.16546504199504852\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 264649, rl-loss: 0.01545767392963171\n",
            "----------------------------------------\n",
            "  timestep     |  5400\n",
            "  reward       |  26.499874988905585\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 265200, rl-loss: 0.12900295853614807\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 266200, rl-loss: 0.020024066790938377\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 267200, rl-loss: 0.01178038027137518\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 268200, rl-loss: 0.14708025753498077\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 269200, rl-loss: 0.10765155404806137\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 270200, rl-loss: 0.0293930284678936\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 271200, rl-loss: 0.14078760147094727\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 272200, rl-loss: 0.10848876088857651\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 273200, rl-loss: 0.16945406794548035\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 274200, rl-loss: 0.13326005637645721\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 274449, rl-loss: 0.06692536920309067\n",
            "----------------------------------------\n",
            "  timestep     |  5600\n",
            "  reward       |  26.527066159078903\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 275200, rl-loss: 0.13386598229408264\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 276200, rl-loss: 0.02081037312746048\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 277200, rl-loss: 0.03297026827931404\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 278200, rl-loss: 0.11729364842176437\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 279200, rl-loss: 0.1423179805278778\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 280200, rl-loss: 0.022877998650074005\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 281200, rl-loss: 0.12619619071483612\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 282200, rl-loss: 0.11817419528961182\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 283200, rl-loss: 0.09887320548295975\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 284200, rl-loss: 0.017509302124381065\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 284249, rl-loss: 0.024586817249655724\n",
            "----------------------------------------\n",
            "  timestep     |  5800\n",
            "  reward       |  26.572419111079952\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 285200, rl-loss: 0.1255539059638977\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 286200, rl-loss: 0.19677576422691345\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 287200, rl-loss: 0.1141948476433754\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 288200, rl-loss: 0.013933347538113594\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 289200, rl-loss: 0.011711061000823975\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 290200, rl-loss: 0.2524401545524597\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 291200, rl-loss: 0.15212799608707428\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 292200, rl-loss: 0.016661718487739563\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 293200, rl-loss: 0.014198224991559982\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 294049, rl-loss: 0.010591700673103333\n",
            "----------------------------------------\n",
            "  timestep     |  6000\n",
            "  reward       |  26.420520570264348\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 294200, rl-loss: 0.018960099667310715\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 295200, rl-loss: 0.01584249548614025\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 296200, rl-loss: 0.044814400374889374\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 297200, rl-loss: 0.1236632838845253\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 298200, rl-loss: 0.14814843237400055\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 299200, rl-loss: 0.14969468116760254\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 300200, rl-loss: 0.014818700030446053\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 301200, rl-loss: 0.014359716325998306\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 302200, rl-loss: 0.1491519957780838\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 303200, rl-loss: 0.04123957082629204\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 303849, rl-loss: 0.058793310075998306\n",
            "----------------------------------------\n",
            "  timestep     |  6200\n",
            "  reward       |  26.516032815901053\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 304200, rl-loss: 0.01678461581468582\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 305200, rl-loss: 0.2458362728357315\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 306200, rl-loss: 0.018253367394208908\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 307200, rl-loss: 0.25420039892196655\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 308200, rl-loss: 0.15802966058254242\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 309200, rl-loss: 0.02299429476261139\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 310200, rl-loss: 0.22972902655601501\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 311200, rl-loss: 0.13741867244243622\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 312200, rl-loss: 0.021929338574409485\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 313200, rl-loss: 0.018327683210372925\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 313649, rl-loss: 0.2681696116924286\n",
            "----------------------------------------\n",
            "  timestep     |  6400\n",
            "  reward       |  26.458327670242316\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 314200, rl-loss: 0.13743016123771667\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 315200, rl-loss: 0.01808440499007702\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 316200, rl-loss: 0.11303146928548813\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 317200, rl-loss: 0.04492057114839554\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 318200, rl-loss: 0.16314741969108582\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 319200, rl-loss: 0.32609906792640686\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 320200, rl-loss: 0.2784848213195801\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 321200, rl-loss: 0.01657870225608349\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 322200, rl-loss: 0.27932578325271606\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 323200, rl-loss: 0.009666933678090572\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 323449, rl-loss: 0.14707931876182556\n",
            "----------------------------------------\n",
            "  timestep     |  6600\n",
            "  reward       |  26.51775965748962\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 324200, rl-loss: 0.018838945776224136\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 325200, rl-loss: 0.019208714365959167\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 326200, rl-loss: 0.03531255945563316\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 327200, rl-loss: 0.26924458146095276\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 328200, rl-loss: 0.014434820972383022\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 329200, rl-loss: 0.13007752597332\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 330200, rl-loss: 0.17962494492530823\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 331200, rl-loss: 0.014249246567487717\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 332200, rl-loss: 0.13335353136062622\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 333200, rl-loss: 0.013619238510727882\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 333249, rl-loss: 0.14649078249931335\n",
            "----------------------------------------\n",
            "  timestep     |  6800\n",
            "  reward       |  26.483729819987126\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 334200, rl-loss: 0.015446195378899574\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 335200, rl-loss: 0.017639709636569023\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 336200, rl-loss: 0.3959554433822632\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 337200, rl-loss: 0.16212044656276703\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 338200, rl-loss: 0.141374871134758\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 339200, rl-loss: 0.01417420245707035\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 340200, rl-loss: 0.019015267491340637\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 341200, rl-loss: 0.13811619579792023\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 342200, rl-loss: 0.013419892638921738\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 343049, rl-loss: 0.019579922780394554\n",
            "----------------------------------------\n",
            "  timestep     |  7000\n",
            "  reward       |  26.507909022366807\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 343200, rl-loss: 0.012364338152110577\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 344200, rl-loss: 0.13915295898914337\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 345200, rl-loss: 0.05833187326788902\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 346200, rl-loss: 0.017004482448101044\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 347200, rl-loss: 0.30303072929382324\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 348200, rl-loss: 0.018116632476449013\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 349200, rl-loss: 0.49501076340675354\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 350200, rl-loss: 0.15131373703479767\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 351200, rl-loss: 0.26247522234916687\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 352200, rl-loss: 0.14504823088645935\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 352849, rl-loss: 0.022557904943823814\n",
            "----------------------------------------\n",
            "  timestep     |  7200\n",
            "  reward       |  26.54885186930366\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 353200, rl-loss: 0.023964906111359596\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 354200, rl-loss: 0.014750640839338303\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 355200, rl-loss: 0.1482727825641632\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 356200, rl-loss: 0.1931207925081253\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 357200, rl-loss: 0.052527766674757004\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 358200, rl-loss: 0.012407714501023293\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 359200, rl-loss: 0.013895956799387932\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 360200, rl-loss: 0.026218079030513763\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 361200, rl-loss: 0.01813969388604164\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 362200, rl-loss: 0.14909052848815918\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 362649, rl-loss: 0.14389154314994812\n",
            "----------------------------------------\n",
            "  timestep     |  7400\n",
            "  reward       |  26.48726938146829\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 363200, rl-loss: 0.01469441782683134\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 364200, rl-loss: 0.048502642661333084\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 365200, rl-loss: 0.27517783641815186\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 366200, rl-loss: 0.1800183355808258\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 367200, rl-loss: 0.1501217484474182\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 368200, rl-loss: 0.20097151398658752\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 369200, rl-loss: 0.014011109247803688\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 370200, rl-loss: 0.014808391220867634\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 371200, rl-loss: 0.019811447709798813\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 372200, rl-loss: 0.15033957362174988\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 372449, rl-loss: 0.013110421597957611\n",
            "----------------------------------------\n",
            "  timestep     |  7600\n",
            "  reward       |  26.49021061894474\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 373200, rl-loss: 0.16998346149921417\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 374200, rl-loss: 0.01582903042435646\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 375200, rl-loss: 0.021027449518442154\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 376200, rl-loss: 0.03517213836312294\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 377200, rl-loss: 0.016037948429584503\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 378200, rl-loss: 0.1488434374332428\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 379200, rl-loss: 0.16720405220985413\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 380200, rl-loss: 0.19392132759094238\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 381200, rl-loss: 0.33378949761390686\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 382200, rl-loss: 0.1778324842453003\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 382249, rl-loss: 0.014316733926534653\n",
            "----------------------------------------\n",
            "  timestep     |  7800\n",
            "  reward       |  26.600481254766997\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 383200, rl-loss: 0.014574614353477955\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 384200, rl-loss: 0.14131848514080048\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 385200, rl-loss: 0.4662599563598633\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 386200, rl-loss: 0.05474518984556198\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 387200, rl-loss: 0.142589271068573\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 388200, rl-loss: 0.016123387962579727\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 389200, rl-loss: 0.2808645963668823\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 390200, rl-loss: 0.09789048135280609\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 391200, rl-loss: 0.1600005030632019\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 392049, rl-loss: 0.04093584045767784\n",
            "----------------------------------------\n",
            "  timestep     |  8000\n",
            "  reward       |  26.459747693531167\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 392200, rl-loss: 0.17342990636825562\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 393200, rl-loss: 0.16941894590854645\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 394200, rl-loss: 0.18271712958812714\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 395200, rl-loss: 0.16318225860595703\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 396200, rl-loss: 0.3403695821762085\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 397200, rl-loss: 0.13221433758735657\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 398200, rl-loss: 0.016131090000271797\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 399200, rl-loss: 0.03673084080219269\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 400200, rl-loss: 0.018976055085659027\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 401200, rl-loss: 0.028492771089076996\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 401849, rl-loss: 0.023351134732365608\n",
            "----------------------------------------\n",
            "  timestep     |  8200\n",
            "  reward       |  26.408680064853193\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 402200, rl-loss: 0.025358980521559715\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 403200, rl-loss: 0.19254891574382782\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 404200, rl-loss: 0.024652011692523956\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 405200, rl-loss: 0.022621624171733856\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 406200, rl-loss: 0.16857069730758667\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 407200, rl-loss: 0.014107991941273212\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 408200, rl-loss: 0.17360419034957886\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 409200, rl-loss: 0.023407138884067535\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 410200, rl-loss: 0.3189736306667328\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 411200, rl-loss: 0.3489530682563782\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 411649, rl-loss: 0.1791372299194336\n",
            "----------------------------------------\n",
            "  timestep     |  8400\n",
            "  reward       |  26.532711401240554\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 412200, rl-loss: 0.17107270658016205\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 413200, rl-loss: 0.15260811150074005\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 414200, rl-loss: 0.32015490531921387\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 415200, rl-loss: 0.02189437486231327\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 416200, rl-loss: 0.02755594067275524\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 417200, rl-loss: 0.015614122152328491\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 418200, rl-loss: 0.360446035861969\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 419200, rl-loss: 0.4940461814403534\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 420200, rl-loss: 0.03852773457765579\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 421200, rl-loss: 0.017881538718938828\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 421449, rl-loss: 0.18641269207000732\n",
            "----------------------------------------\n",
            "  timestep     |  8600\n",
            "  reward       |  26.44719177935313\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 422200, rl-loss: 0.1703677475452423\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 423200, rl-loss: 0.012801643460988998\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 424200, rl-loss: 0.01010383851826191\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 425200, rl-loss: 0.013500534929335117\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 426200, rl-loss: 0.19341528415679932\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 427200, rl-loss: 0.01755274273455143\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 428200, rl-loss: 0.16185125708580017\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 429200, rl-loss: 0.17120274901390076\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 430200, rl-loss: 0.013318302109837532\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 431200, rl-loss: 0.014456641860306263\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 431249, rl-loss: 0.2000913918018341\n",
            "----------------------------------------\n",
            "  timestep     |  8800\n",
            "  reward       |  26.404597103778592\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 432200, rl-loss: 0.03450874239206314\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 433200, rl-loss: 0.19864588975906372\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 434200, rl-loss: 0.01787114515900612\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 435200, rl-loss: 0.014149937778711319\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 436200, rl-loss: 0.014496185816824436\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 437200, rl-loss: 0.024349302053451538\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 438200, rl-loss: 0.016161294654011726\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 439200, rl-loss: 0.3192625641822815\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 440200, rl-loss: 0.019414519891142845\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 441049, rl-loss: 0.16492179036140442\n",
            "----------------------------------------\n",
            "  timestep     |  9000\n",
            "  reward       |  26.63066116100787\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 441200, rl-loss: 0.17595554888248444\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 442200, rl-loss: 0.017523709684610367\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 443200, rl-loss: 0.19444091618061066\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 444200, rl-loss: 0.5518597960472107\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 445200, rl-loss: 0.17125868797302246\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 446200, rl-loss: 0.6610965728759766\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 447200, rl-loss: 0.014650991186499596\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 448200, rl-loss: 0.18646951019763947\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 449200, rl-loss: 0.18802249431610107\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 450200, rl-loss: 0.018314586952328682\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 450849, rl-loss: 0.32424622774124146\n",
            "----------------------------------------\n",
            "  timestep     |  9200\n",
            "  reward       |  26.427144633157116\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 451200, rl-loss: 0.19235403835773468\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 452200, rl-loss: 0.15235449373722076\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 453200, rl-loss: 0.17618101835250854\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 454200, rl-loss: 0.16978153586387634\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 455200, rl-loss: 0.020946117118000984\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 456200, rl-loss: 0.01887393929064274\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 457200, rl-loss: 0.1967633217573166\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 458200, rl-loss: 0.3417998254299164\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 459200, rl-loss: 0.03849705681204796\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 460200, rl-loss: 0.04124101996421814\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 460649, rl-loss: 0.1991344541311264\n",
            "----------------------------------------\n",
            "  timestep     |  9400\n",
            "  reward       |  26.512477987759798\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 461200, rl-loss: 0.08989104628562927\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 462200, rl-loss: 0.2178790420293808\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 463200, rl-loss: 0.01558152586221695\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 464200, rl-loss: 0.020660459995269775\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 465200, rl-loss: 0.019480373710393906\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 466200, rl-loss: 0.17399336397647858\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 467200, rl-loss: 0.013243108987808228\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 468200, rl-loss: 0.026542380452156067\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 469200, rl-loss: 0.017718564718961716\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 470200, rl-loss: 0.18253912031650543\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 470449, rl-loss: 0.17254596948623657\n",
            "----------------------------------------\n",
            "  timestep     |  9600\n",
            "  reward       |  26.5125660228221\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 471200, rl-loss: 0.18944916129112244\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 472200, rl-loss: 0.016952717676758766\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 473200, rl-loss: 0.053270675241947174\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 474200, rl-loss: 0.014345386065542698\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 475200, rl-loss: 0.20501503348350525\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 476200, rl-loss: 0.20077459514141083\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 477200, rl-loss: 0.24729758501052856\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 478200, rl-loss: 0.019747456535696983\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 479200, rl-loss: 0.0457795150578022\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 480200, rl-loss: 0.18609116971492767\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 480249, rl-loss: 0.3651602864265442\n",
            "----------------------------------------\n",
            "  timestep     |  9800\n",
            "  reward       |  26.431254810990332\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 481200, rl-loss: 0.025231221690773964\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 482200, rl-loss: 0.013928599655628204\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 483200, rl-loss: 0.017547663301229477\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 484200, rl-loss: 0.01175870094448328\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 485200, rl-loss: 0.2266639620065689\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 486200, rl-loss: 0.5290563106536865\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 487200, rl-loss: 0.07821574807167053\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 488200, rl-loss: 0.018881352618336678\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 489200, rl-loss: 0.18829163908958435\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 490000, rl-loss: 0.01399365533143282"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRzmdZr-VL5A",
        "outputId": "6655dfef-8f87-45ca-b47f-a0e0b16fc7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "logger.close_files()\n",
        "logger.plot('DQN')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./experiments/blackjack_dqn_result/performance.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awlrQQCUpEJd",
        "outputId": "b85754e1-917c-4d83-942c-45718d540f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.download('./experiments/blackjack_dqn_result/performance.csv') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3d19d518c153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./experiments/blackjack_dqn_result/performance.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: ./experiments/blackjack_dqn_result/performance.csv"
          ]
        }
      ]
    }
  ]
}